# Load the whisper model

model = whisper.load_model("base")
model.device

#Download a test audio

#@title Download a test audio
from IPython.display import clear_output
!wget -O audio.mp3 http://www.moviesoundclips.net/movies1/darkknightrises/darkness.mp3
clear_output()
!ls | grep '.mp3'

audio = "audio.mp3"

# Play audio
Audio(audio)

# Load and process the audio file
audio_data = whisper.load_audio(audio)
audio_data = whisper.pad_or_trim(audio_data)

# Make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio_data).to(model.device)
mel.shape

# Visualize spectogram
plt.figure(figsize=(10, 5))
plt.imshow(mel)
plt.show()

# Detect the spoken language
_, probs = model.detect_language(mel)
detected_language = max(probs, key=probs.get)

print(f"Detected language: {detected_language}")

# This cell will take some time to run on cpu
options = whisper.DecodingOptions(language='en')             # Use the selected language for transcription
result = whisper.decode(model, mel, options)
result.text

